{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-08T18:10:19.069076Z","iopub.execute_input":"2021-09-08T18:10:19.069555Z","iopub.status.idle":"2021-09-08T18:10:19.082274Z","shell.execute_reply.started":"2021-09-08T18:10:19.069482Z","shell.execute_reply":"2021-09-08T18:10:19.080429Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/tabular-playground-series-sep-2021/train.csv\n/kaggle/input/tabular-playground-series-sep-2021/test.csv\n/kaggle/input/tabular-playground-series-sep-2021/sample_solution.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Table of content\n- A. [Prepare data with Null values for the example](#section-a)\n- B. [Methods to handle the missing values](#section-b)\n    - 1. [Drop the data](#section-drop)\n    - 2. [Fill the places with any of 5 Ms (Mean/Median/Mode/Max/Min)](#section-5m)\n    - 3. [Fill the place with some constant value](#section-constant)\n    - 4. [Predict the missing values](#section-predict)\n        - 4.1.[Model-based strategy](#section-model-based)\n        - 4.2.[Progressive model-based strategy](#section-progressive-model-based)\n    - 5. [Use models which support missing values](#section-model)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-a\"></a>\n# A. Prepare data with Null values for the example\n","metadata":{}},{"cell_type":"code","source":"data_ = pd.read_csv(\"../input/tabular-playground-series-sep-2021/train.csv\",index_col = 0)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:11:00.738932Z","iopub.execute_input":"2021-09-08T18:11:00.739216Z","iopub.status.idle":"2021-09-08T18:11:23.665100Z","shell.execute_reply.started":"2021-09-08T18:11:00.739192Z","shell.execute_reply":"2021-09-08T18:11:23.663741Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# keeping just 50 rows for my example.\ndef create_data():\n    data = data_.iloc[0:50,30:50]\n    return data\ndata = create_data()\nprint(\"Shape of the data------>{}\".format(data.shape))\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:12:33.250933Z","iopub.execute_input":"2021-09-08T18:12:33.251469Z","iopub.status.idle":"2021-09-08T18:12:33.308589Z","shell.execute_reply.started":"2021-09-08T18:12:33.251439Z","shell.execute_reply":"2021-09-08T18:12:33.307824Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Shape of the data------>(50, 20)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        f31       f32       f33       f34           f35       f36      f37  \\\nid                                                                           \n0   0.75050   18509.0  146820.0 -0.000276  1.090600e+16  1705.400   414.29   \n1   1.50330  238000.0   21440.0 -0.001344  3.079400e+16   229.100   844.82   \n2   1.13180   27940.0  862460.0 -0.002207  5.849100e+13  -897.840      NaN   \n3   0.98941  301200.0       NaN -0.000007 -9.299200e+13   -10.818  1020.30   \n4   0.97413  142620.0  231350.0  0.001257  1.012500e+16    51.508   293.76   \n\n       f38     f39       f40      f41       f42      f43      f44       f45  \\\nid                                                                            \n0   3.5392  1888.0  0.968930  18.3880 -0.001583   7.7059   5.9325  0.025693   \n1   1.4680  4726.5  0.915380  -1.5321  0.982600   7.1112   2.0797  0.042321   \n2   1.3561  3063.4  0.086232  16.1060  0.001481  11.4760   5.3430  0.012162   \n3   2.9553  3342.5 -0.000372  17.0110  0.095268   5.7448  15.8830  0.037934   \n4   1.3351  3042.1  0.006791  94.8890  0.917090   8.7369      NaN  0.020281   \n\n       f46      f47      f48       f49       f50  \nid                                                \n0   4.5604  0.61122  10.7950  0.341930  0.235010  \n1   4.2523  0.41871   5.4499  0.012737  0.386470  \n2   4.1018 -0.88270   8.1228 -0.676690  0.337700  \n3   4.4860 -0.88909   8.4384 -1.189800  0.001391  \n4   3.9115  0.65634   6.1410 -1.089600  0.247940  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f31</th>\n      <th>f32</th>\n      <th>f33</th>\n      <th>f34</th>\n      <th>f35</th>\n      <th>f36</th>\n      <th>f37</th>\n      <th>f38</th>\n      <th>f39</th>\n      <th>f40</th>\n      <th>f41</th>\n      <th>f42</th>\n      <th>f43</th>\n      <th>f44</th>\n      <th>f45</th>\n      <th>f46</th>\n      <th>f47</th>\n      <th>f48</th>\n      <th>f49</th>\n      <th>f50</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.75050</td>\n      <td>18509.0</td>\n      <td>146820.0</td>\n      <td>-0.000276</td>\n      <td>1.090600e+16</td>\n      <td>1705.400</td>\n      <td>414.29</td>\n      <td>3.5392</td>\n      <td>1888.0</td>\n      <td>0.968930</td>\n      <td>18.3880</td>\n      <td>-0.001583</td>\n      <td>7.7059</td>\n      <td>5.9325</td>\n      <td>0.025693</td>\n      <td>4.5604</td>\n      <td>0.61122</td>\n      <td>10.7950</td>\n      <td>0.341930</td>\n      <td>0.235010</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.50330</td>\n      <td>238000.0</td>\n      <td>21440.0</td>\n      <td>-0.001344</td>\n      <td>3.079400e+16</td>\n      <td>229.100</td>\n      <td>844.82</td>\n      <td>1.4680</td>\n      <td>4726.5</td>\n      <td>0.915380</td>\n      <td>-1.5321</td>\n      <td>0.982600</td>\n      <td>7.1112</td>\n      <td>2.0797</td>\n      <td>0.042321</td>\n      <td>4.2523</td>\n      <td>0.41871</td>\n      <td>5.4499</td>\n      <td>0.012737</td>\n      <td>0.386470</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.13180</td>\n      <td>27940.0</td>\n      <td>862460.0</td>\n      <td>-0.002207</td>\n      <td>5.849100e+13</td>\n      <td>-897.840</td>\n      <td>NaN</td>\n      <td>1.3561</td>\n      <td>3063.4</td>\n      <td>0.086232</td>\n      <td>16.1060</td>\n      <td>0.001481</td>\n      <td>11.4760</td>\n      <td>5.3430</td>\n      <td>0.012162</td>\n      <td>4.1018</td>\n      <td>-0.88270</td>\n      <td>8.1228</td>\n      <td>-0.676690</td>\n      <td>0.337700</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.98941</td>\n      <td>301200.0</td>\n      <td>NaN</td>\n      <td>-0.000007</td>\n      <td>-9.299200e+13</td>\n      <td>-10.818</td>\n      <td>1020.30</td>\n      <td>2.9553</td>\n      <td>3342.5</td>\n      <td>-0.000372</td>\n      <td>17.0110</td>\n      <td>0.095268</td>\n      <td>5.7448</td>\n      <td>15.8830</td>\n      <td>0.037934</td>\n      <td>4.4860</td>\n      <td>-0.88909</td>\n      <td>8.4384</td>\n      <td>-1.189800</td>\n      <td>0.001391</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.97413</td>\n      <td>142620.0</td>\n      <td>231350.0</td>\n      <td>0.001257</td>\n      <td>1.012500e+16</td>\n      <td>51.508</td>\n      <td>293.76</td>\n      <td>1.3351</td>\n      <td>3042.1</td>\n      <td>0.006791</td>\n      <td>94.8890</td>\n      <td>0.917090</td>\n      <td>8.7369</td>\n      <td>NaN</td>\n      <td>0.020281</td>\n      <td>3.9115</td>\n      <td>0.65634</td>\n      <td>6.1410</td>\n      <td>-1.089600</td>\n      <td>0.247940</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# find the missing values.\nfeatures = data.columns.tolist()[0:-1]\n\n\n#find the missing values w.r.t. column\ncolum_missing = data.isnull().sum()\n# find the missing values w.r.t. row(number of missing values in the particular row)\nrow_missing = data[features].isnull().sum(axis=1)\n\n# add the missing values to row to the dataframe as a new value\ndata['no_of_missing_data'] = row_missing","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:12:33.816844Z","iopub.execute_input":"2021-09-08T18:12:33.817312Z","iopub.status.idle":"2021-09-08T18:12:33.827271Z","shell.execute_reply.started":"2021-09-08T18:12:33.817282Z","shell.execute_reply":"2021-09-08T18:12:33.826314Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"colum_missing","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:12:35.139782Z","iopub.execute_input":"2021-09-08T18:12:35.140307Z","iopub.status.idle":"2021-09-08T18:12:35.149616Z","shell.execute_reply.started":"2021-09-08T18:12:35.140273Z","shell.execute_reply":"2021-09-08T18:12:35.148836Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"f31    0\nf32    1\nf33    2\nf34    1\nf35    1\nf36    2\nf37    3\nf38    0\nf39    0\nf40    0\nf41    1\nf42    1\nf43    1\nf44    2\nf45    2\nf46    0\nf47    0\nf48    1\nf49    0\nf50    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Total number of missing values in training dataset---->{data.shape[0]}\")\n\n# compare this to the whole data\nno_of_missing_rows = (data['no_of_missing_data'] != 0).sum()\nprint(\"\\n{0:{fill}{align}80}\\n\".format(\" Data Summary \" , fill = \"=\", align = \"^\"))\nprint(f\"Total rows -----------------------> {data.shape[0]}\\nNumber of rows has missing data---> {no_of_missing_rows}\\n{'-'*50}\\nNumber of rows has full data--------> {data.shape[0] - no_of_missing_rows}\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:12:35.946502Z","iopub.execute_input":"2021-09-08T18:12:35.946987Z","iopub.status.idle":"2021-09-08T18:12:35.974859Z","shell.execute_reply.started":"2021-09-08T18:12:35.946965Z","shell.execute_reply":"2021-09-08T18:12:35.973633Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Total number of missing values in training dataset---->50\n\n================================= Data Summary =================================\n\nTotal rows -----------------------> 50\nNumber of rows has missing data---> 17\n--------------------------------------------------\nNumber of rows has full data--------> 33\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"section-b\"></a>\n# B. Methods to handle the missing values\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-drop\"></a>\n## 1. Drop the data","metadata":{}},{"cell_type":"markdown","source":"In this method, we simply delete the rows or features/columns which has the Null value. We will delete a row if there are more missing values (say 70-75%) same goes for the columns. This is only preferred to use when we have enough samples in the dataset.  We can delete a feature/column when it has less feature importance over prediction. One has to make sure there is no add of bias, after we have removed the data.\n\nBuild-in functions:\n- dropna() --> https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\n\nPros: \n1. Removing this unwanted data sometimes make our model more accurate.\n2. Deleting a column with less importance is better, since there is no use of keeping this with full of null values and no use of speding time handling this.\n\nCons:\n1. Loss of information.\n2. Redure prediction accuracy --> when we have a big number of missing values.","metadata":{}},{"cell_type":"code","source":"# Example.\nrow_drop = create_data()\n# Drop the rows with null value\nrow_drop.dropna( how = 'any' , inplace = True) # we can change how to any/all . if 'all' the row will be deleted when it has all values as null values.\nrow_drop.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:12:38.755192Z","iopub.execute_input":"2021-09-08T18:12:38.755479Z","iopub.status.idle":"2021-09-08T18:12:38.765552Z","shell.execute_reply.started":"2021-09-08T18:12:38.755456Z","shell.execute_reply":"2021-09-08T18:12:38.764503Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"f31    0\nf32    0\nf33    0\nf34    0\nf35    0\nf36    0\nf37    0\nf38    0\nf39    0\nf40    0\nf41    0\nf42    0\nf43    0\nf44    0\nf45    0\nf46    0\nf47    0\nf48    0\nf49    0\nf50    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Drop the column --> here the f37  has '3' values. we can delete if we want to..\ndata = create_data()\ndata = data_.drop('f37', axis = 1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:12:39.826805Z","iopub.execute_input":"2021-09-08T18:12:39.827208Z","iopub.status.idle":"2021-09-08T18:12:40.046304Z","shell.execute_reply.started":"2021-09-08T18:12:39.827183Z","shell.execute_reply":"2021-09-08T18:12:40.045098Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"         f1        f2         f3        f4       f5        f6       f7  \\\nid                                                                       \n0   0.10859  0.004314    -37.566  0.017364  0.28915 -10.25100   135.12   \n1   0.10090  0.299610  11822.000  0.276500  0.45970  -0.83733  1721.90   \n2   0.17803 -0.006980    907.270  0.272140  0.45948   0.17327  2298.00   \n3   0.15236  0.007259    780.100  0.025179  0.51947   7.49140   112.51   \n4   0.11623  0.502900   -109.150  0.297910  0.34490  -0.40932  2538.90   \n\n          f8            f9        f10  ...     f110    f111     f112  \\\nid                                     ...                             \n0   168900.0  3.992400e+14     86.489  ... -12.2280  1.7482  1.90960   \n1   119810.0  3.874100e+15   9953.600  ... -56.7580  4.1684  0.34808   \n2   360650.0  1.224500e+13  15827.000  ...  -5.7688  1.2042  0.26290   \n3   259490.0  7.781400e+13    -36.837  ... -34.8580  2.0694  0.79631   \n4    65332.0  1.907200e+15    144.120  ... -13.6410  1.5298  1.14640   \n\n        f113      f114    f115          f116    f117     f118  claim  \nid                                                                    \n0   -7.11570   4378.80  1.2096  8.613400e+14   140.1  1.01770      1  \n1    4.14200    913.23  1.2464  7.575100e+15  1861.0  0.28359      0  \n2    8.13120  45119.00  1.1764  3.218100e+14  3838.2  0.40690      1  \n3  -16.33600   4952.40  1.1784  4.533000e+12  4889.1  0.51486      1  \n4   -0.43124   3856.50  1.4830 -8.991300e+12     NaN  0.23049      1  \n\n[5 rows x 118 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>f10</th>\n      <th>...</th>\n      <th>f110</th>\n      <th>f111</th>\n      <th>f112</th>\n      <th>f113</th>\n      <th>f114</th>\n      <th>f115</th>\n      <th>f116</th>\n      <th>f117</th>\n      <th>f118</th>\n      <th>claim</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.10859</td>\n      <td>0.004314</td>\n      <td>-37.566</td>\n      <td>0.017364</td>\n      <td>0.28915</td>\n      <td>-10.25100</td>\n      <td>135.12</td>\n      <td>168900.0</td>\n      <td>3.992400e+14</td>\n      <td>86.489</td>\n      <td>...</td>\n      <td>-12.2280</td>\n      <td>1.7482</td>\n      <td>1.90960</td>\n      <td>-7.11570</td>\n      <td>4378.80</td>\n      <td>1.2096</td>\n      <td>8.613400e+14</td>\n      <td>140.1</td>\n      <td>1.01770</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.10090</td>\n      <td>0.299610</td>\n      <td>11822.000</td>\n      <td>0.276500</td>\n      <td>0.45970</td>\n      <td>-0.83733</td>\n      <td>1721.90</td>\n      <td>119810.0</td>\n      <td>3.874100e+15</td>\n      <td>9953.600</td>\n      <td>...</td>\n      <td>-56.7580</td>\n      <td>4.1684</td>\n      <td>0.34808</td>\n      <td>4.14200</td>\n      <td>913.23</td>\n      <td>1.2464</td>\n      <td>7.575100e+15</td>\n      <td>1861.0</td>\n      <td>0.28359</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.17803</td>\n      <td>-0.006980</td>\n      <td>907.270</td>\n      <td>0.272140</td>\n      <td>0.45948</td>\n      <td>0.17327</td>\n      <td>2298.00</td>\n      <td>360650.0</td>\n      <td>1.224500e+13</td>\n      <td>15827.000</td>\n      <td>...</td>\n      <td>-5.7688</td>\n      <td>1.2042</td>\n      <td>0.26290</td>\n      <td>8.13120</td>\n      <td>45119.00</td>\n      <td>1.1764</td>\n      <td>3.218100e+14</td>\n      <td>3838.2</td>\n      <td>0.40690</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.15236</td>\n      <td>0.007259</td>\n      <td>780.100</td>\n      <td>0.025179</td>\n      <td>0.51947</td>\n      <td>7.49140</td>\n      <td>112.51</td>\n      <td>259490.0</td>\n      <td>7.781400e+13</td>\n      <td>-36.837</td>\n      <td>...</td>\n      <td>-34.8580</td>\n      <td>2.0694</td>\n      <td>0.79631</td>\n      <td>-16.33600</td>\n      <td>4952.40</td>\n      <td>1.1784</td>\n      <td>4.533000e+12</td>\n      <td>4889.1</td>\n      <td>0.51486</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.11623</td>\n      <td>0.502900</td>\n      <td>-109.150</td>\n      <td>0.297910</td>\n      <td>0.34490</td>\n      <td>-0.40932</td>\n      <td>2538.90</td>\n      <td>65332.0</td>\n      <td>1.907200e+15</td>\n      <td>144.120</td>\n      <td>...</td>\n      <td>-13.6410</td>\n      <td>1.5298</td>\n      <td>1.14640</td>\n      <td>-0.43124</td>\n      <td>3856.50</td>\n      <td>1.4830</td>\n      <td>-8.991300e+12</td>\n      <td>NaN</td>\n      <td>0.23049</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 118 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id=\"section-5m\"></a>\n## 2. Fill the places with any of 5 Ms (Mean/Median/Mode/Max/Min)","metadata":{}},{"cell_type":"markdown","source":"In this method, we can replace the null value with some approximations (Average(mean), Median, Mode, Min, and Max). This method can be used with the numerical columns. Even this is an approximate calculation for the null value, It is better than deleting the rows and columns. The Mean, Median, Mode are a statistical approach to handling the missing values. \n\nBuild-in functions:\n- fillna() --> https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html ( with different methods)\n- SimpleImputer--> https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n\nPros:\n1. There is no loss of information.\n2. Better approach with small dataset.\n\nCons:\n1. Imputing the approximations add variance and bias.","metadata":{}},{"cell_type":"markdown","source":"### Example with SimpleImputer() ---> for mean / median / mode\nChnage the strategy to ----> mean / median / most_frequent(mode) --------->median is the most frequently used strategy","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\n\ndata_before = data.copy()\n#print(data_before.isnull().sum())\ndata_after = pd.DataFrame(imputer.fit_transform(data_before))\n#print(data_after.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:12:43.820104Z","iopub.execute_input":"2021-09-08T18:12:43.820421Z","iopub.status.idle":"2021-09-08T18:12:48.095667Z","shell.execute_reply.started":"2021-09-08T18:12:43.820393Z","shell.execute_reply":"2021-09-08T18:12:48.094685Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Example with replace() ---> for max / min","metadata":{}},{"cell_type":"code","source":"data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:12:50.297712Z","iopub.execute_input":"2021-09-08T18:12:50.298016Z","iopub.status.idle":"2021-09-08T18:12:50.326171Z","shell.execute_reply.started":"2021-09-08T18:12:50.297992Z","shell.execute_reply":"2021-09-08T18:12:50.325307Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"         f1        f2         f3        f4       f5        f6       f7  \\\nid                                                                       \n0   0.10859  0.004314    -37.566  0.017364  0.28915 -10.25100   135.12   \n1   0.10090  0.299610  11822.000  0.276500  0.45970  -0.83733  1721.90   \n2   0.17803 -0.006980    907.270  0.272140  0.45948   0.17327  2298.00   \n3   0.15236  0.007259    780.100  0.025179  0.51947   7.49140   112.51   \n4   0.11623  0.502900   -109.150  0.297910  0.34490  -0.40932  2538.90   \n\n          f8            f9        f10  ...     f110    f111     f112  \\\nid                                     ...                             \n0   168900.0  3.992400e+14     86.489  ... -12.2280  1.7482  1.90960   \n1   119810.0  3.874100e+15   9953.600  ... -56.7580  4.1684  0.34808   \n2   360650.0  1.224500e+13  15827.000  ...  -5.7688  1.2042  0.26290   \n3   259490.0  7.781400e+13    -36.837  ... -34.8580  2.0694  0.79631   \n4    65332.0  1.907200e+15    144.120  ... -13.6410  1.5298  1.14640   \n\n        f113      f114    f115          f116    f117     f118  claim  \nid                                                                    \n0   -7.11570   4378.80  1.2096  8.613400e+14   140.1  1.01770      1  \n1    4.14200    913.23  1.2464  7.575100e+15  1861.0  0.28359      0  \n2    8.13120  45119.00  1.1764  3.218100e+14  3838.2  0.40690      1  \n3  -16.33600   4952.40  1.1784  4.533000e+12  4889.1  0.51486      1  \n4   -0.43124   3856.50  1.4830 -8.991300e+12     NaN  0.23049      1  \n\n[5 rows x 118 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>f10</th>\n      <th>...</th>\n      <th>f110</th>\n      <th>f111</th>\n      <th>f112</th>\n      <th>f113</th>\n      <th>f114</th>\n      <th>f115</th>\n      <th>f116</th>\n      <th>f117</th>\n      <th>f118</th>\n      <th>claim</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.10859</td>\n      <td>0.004314</td>\n      <td>-37.566</td>\n      <td>0.017364</td>\n      <td>0.28915</td>\n      <td>-10.25100</td>\n      <td>135.12</td>\n      <td>168900.0</td>\n      <td>3.992400e+14</td>\n      <td>86.489</td>\n      <td>...</td>\n      <td>-12.2280</td>\n      <td>1.7482</td>\n      <td>1.90960</td>\n      <td>-7.11570</td>\n      <td>4378.80</td>\n      <td>1.2096</td>\n      <td>8.613400e+14</td>\n      <td>140.1</td>\n      <td>1.01770</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.10090</td>\n      <td>0.299610</td>\n      <td>11822.000</td>\n      <td>0.276500</td>\n      <td>0.45970</td>\n      <td>-0.83733</td>\n      <td>1721.90</td>\n      <td>119810.0</td>\n      <td>3.874100e+15</td>\n      <td>9953.600</td>\n      <td>...</td>\n      <td>-56.7580</td>\n      <td>4.1684</td>\n      <td>0.34808</td>\n      <td>4.14200</td>\n      <td>913.23</td>\n      <td>1.2464</td>\n      <td>7.575100e+15</td>\n      <td>1861.0</td>\n      <td>0.28359</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.17803</td>\n      <td>-0.006980</td>\n      <td>907.270</td>\n      <td>0.272140</td>\n      <td>0.45948</td>\n      <td>0.17327</td>\n      <td>2298.00</td>\n      <td>360650.0</td>\n      <td>1.224500e+13</td>\n      <td>15827.000</td>\n      <td>...</td>\n      <td>-5.7688</td>\n      <td>1.2042</td>\n      <td>0.26290</td>\n      <td>8.13120</td>\n      <td>45119.00</td>\n      <td>1.1764</td>\n      <td>3.218100e+14</td>\n      <td>3838.2</td>\n      <td>0.40690</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.15236</td>\n      <td>0.007259</td>\n      <td>780.100</td>\n      <td>0.025179</td>\n      <td>0.51947</td>\n      <td>7.49140</td>\n      <td>112.51</td>\n      <td>259490.0</td>\n      <td>7.781400e+13</td>\n      <td>-36.837</td>\n      <td>...</td>\n      <td>-34.8580</td>\n      <td>2.0694</td>\n      <td>0.79631</td>\n      <td>-16.33600</td>\n      <td>4952.40</td>\n      <td>1.1784</td>\n      <td>4.533000e+12</td>\n      <td>4889.1</td>\n      <td>0.51486</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.11623</td>\n      <td>0.502900</td>\n      <td>-109.150</td>\n      <td>0.297910</td>\n      <td>0.34490</td>\n      <td>-0.40932</td>\n      <td>2538.90</td>\n      <td>65332.0</td>\n      <td>1.907200e+15</td>\n      <td>144.120</td>\n      <td>...</td>\n      <td>-13.6410</td>\n      <td>1.5298</td>\n      <td>1.14640</td>\n      <td>-0.43124</td>\n      <td>3856.50</td>\n      <td>1.4830</td>\n      <td>-8.991300e+12</td>\n      <td>NaN</td>\n      <td>0.23049</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 118 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Example to replace with max in that column.\ndata_max = data.copy()\nfor f in data.columns.tolist():\n    data_max[f] = data_max[f].replace(np.NaN, data[f].max())\ndata_max.head(5)    ","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:12:51.296583Z","iopub.execute_input":"2021-09-08T18:12:51.296949Z","iopub.status.idle":"2021-09-08T18:12:52.245344Z","shell.execute_reply.started":"2021-09-08T18:12:51.296920Z","shell.execute_reply":"2021-09-08T18:12:52.243750Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"         f1        f2         f3        f4       f5        f6       f7  \\\nid                                                                       \n0   0.10859  0.004314    -37.566  0.017364  0.28915 -10.25100   135.12   \n1   0.10090  0.299610  11822.000  0.276500  0.45970  -0.83733  1721.90   \n2   0.17803 -0.006980    907.270  0.272140  0.45948   0.17327  2298.00   \n3   0.15236  0.007259    780.100  0.025179  0.51947   7.49140   112.51   \n4   0.11623  0.502900   -109.150  0.297910  0.34490  -0.40932  2538.90   \n\n          f8            f9        f10  ...     f110    f111     f112  \\\nid                                     ...                             \n0   168900.0  3.992400e+14     86.489  ... -12.2280  1.7482  1.90960   \n1   119810.0  3.874100e+15   9953.600  ... -56.7580  4.1684  0.34808   \n2   360650.0  1.224500e+13  15827.000  ...  -5.7688  1.2042  0.26290   \n3   259490.0  7.781400e+13    -36.837  ... -34.8580  2.0694  0.79631   \n4    65332.0  1.907200e+15    144.120  ... -13.6410  1.5298  1.14640   \n\n        f113      f114    f115          f116     f117     f118  claim  \nid                                                                     \n0   -7.11570   4378.80  1.2096  8.613400e+14    140.1  1.01770      1  \n1    4.14200    913.23  1.2464  7.575100e+15   1861.0  0.28359      0  \n2    8.13120  45119.00  1.1764  3.218100e+14   3838.2  0.40690      1  \n3  -16.33600   4952.40  1.1784  4.533000e+12   4889.1  0.51486      1  \n4   -0.43124   3856.50  1.4830 -8.991300e+12  13151.0  0.23049      1  \n\n[5 rows x 118 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>f10</th>\n      <th>...</th>\n      <th>f110</th>\n      <th>f111</th>\n      <th>f112</th>\n      <th>f113</th>\n      <th>f114</th>\n      <th>f115</th>\n      <th>f116</th>\n      <th>f117</th>\n      <th>f118</th>\n      <th>claim</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.10859</td>\n      <td>0.004314</td>\n      <td>-37.566</td>\n      <td>0.017364</td>\n      <td>0.28915</td>\n      <td>-10.25100</td>\n      <td>135.12</td>\n      <td>168900.0</td>\n      <td>3.992400e+14</td>\n      <td>86.489</td>\n      <td>...</td>\n      <td>-12.2280</td>\n      <td>1.7482</td>\n      <td>1.90960</td>\n      <td>-7.11570</td>\n      <td>4378.80</td>\n      <td>1.2096</td>\n      <td>8.613400e+14</td>\n      <td>140.1</td>\n      <td>1.01770</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.10090</td>\n      <td>0.299610</td>\n      <td>11822.000</td>\n      <td>0.276500</td>\n      <td>0.45970</td>\n      <td>-0.83733</td>\n      <td>1721.90</td>\n      <td>119810.0</td>\n      <td>3.874100e+15</td>\n      <td>9953.600</td>\n      <td>...</td>\n      <td>-56.7580</td>\n      <td>4.1684</td>\n      <td>0.34808</td>\n      <td>4.14200</td>\n      <td>913.23</td>\n      <td>1.2464</td>\n      <td>7.575100e+15</td>\n      <td>1861.0</td>\n      <td>0.28359</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.17803</td>\n      <td>-0.006980</td>\n      <td>907.270</td>\n      <td>0.272140</td>\n      <td>0.45948</td>\n      <td>0.17327</td>\n      <td>2298.00</td>\n      <td>360650.0</td>\n      <td>1.224500e+13</td>\n      <td>15827.000</td>\n      <td>...</td>\n      <td>-5.7688</td>\n      <td>1.2042</td>\n      <td>0.26290</td>\n      <td>8.13120</td>\n      <td>45119.00</td>\n      <td>1.1764</td>\n      <td>3.218100e+14</td>\n      <td>3838.2</td>\n      <td>0.40690</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.15236</td>\n      <td>0.007259</td>\n      <td>780.100</td>\n      <td>0.025179</td>\n      <td>0.51947</td>\n      <td>7.49140</td>\n      <td>112.51</td>\n      <td>259490.0</td>\n      <td>7.781400e+13</td>\n      <td>-36.837</td>\n      <td>...</td>\n      <td>-34.8580</td>\n      <td>2.0694</td>\n      <td>0.79631</td>\n      <td>-16.33600</td>\n      <td>4952.40</td>\n      <td>1.1784</td>\n      <td>4.533000e+12</td>\n      <td>4889.1</td>\n      <td>0.51486</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.11623</td>\n      <td>0.502900</td>\n      <td>-109.150</td>\n      <td>0.297910</td>\n      <td>0.34490</td>\n      <td>-0.40932</td>\n      <td>2538.90</td>\n      <td>65332.0</td>\n      <td>1.907200e+15</td>\n      <td>144.120</td>\n      <td>...</td>\n      <td>-13.6410</td>\n      <td>1.5298</td>\n      <td>1.14640</td>\n      <td>-0.43124</td>\n      <td>3856.50</td>\n      <td>1.4830</td>\n      <td>-8.991300e+12</td>\n      <td>13151.0</td>\n      <td>0.23049</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 118 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Above we can see (4,f44) has NaN value in the original data. But after replacement that value is filled with the max() of that particular column.<br>\nChange max() with min() to use min function.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-constant\"></a>\n## 3. Fill the place with some constant value","metadata":{}},{"cell_type":"markdown","source":"In this method, we will add some constant fixed values in place of null values. for example, people's gender has only two categories, some follower classification has 4-5 classes, etc. Here we can add another class/category named 'Unknow' this will add more information to the dataset at the same time prevent information loss. In numerical columns, we can take some constant values to mark these places as null values. We can use -9999 or any number which seems correct for you. <br>\nWill be a  problem for categorical values, since they need to convert into numerical form. We will use some encoding technology for this like \"one-hot encoding\". This will increase the number of columns when we are adding one more class.\n\nBuild-in functions:\n- SimpleImputer--> https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n\nPros:\n1. no loss of information here.\n2. We have more control oue the new class/feature--> we can delete the data belonged to this class or do what ever we want.\n\nCons:\n1. Peoblem in Encoding which may redice our accuracy.","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values = np.nan, strategy = \"constant\", fill_value = -9999)\n\ndata_before = data.copy()\n#print(data_before.isnull().sum())\ndata_after = pd.DataFrame(imputer.fit_transform(data_before))\n#print(data_after.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:12:53.821248Z","iopub.execute_input":"2021-09-08T18:12:53.821601Z","iopub.status.idle":"2021-09-08T18:12:56.427154Z","shell.execute_reply.started":"2021-09-08T18:12:53.821572Z","shell.execute_reply":"2021-09-08T18:12:56.425593Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Example to replace with max in that column.\ndata_max = data.copy()\nfor f in data.columns.tolist():\n    data_max[f] = data_max[f].replace(np.NaN, -9999)\ndata_max.head(5) \n","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:12:58.513896Z","iopub.execute_input":"2021-09-08T18:12:58.514226Z","iopub.status.idle":"2021-09-08T18:12:59.316920Z","shell.execute_reply.started":"2021-09-08T18:12:58.514198Z","shell.execute_reply":"2021-09-08T18:12:59.315195Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"         f1        f2         f3        f4       f5        f6       f7  \\\nid                                                                       \n0   0.10859  0.004314    -37.566  0.017364  0.28915 -10.25100   135.12   \n1   0.10090  0.299610  11822.000  0.276500  0.45970  -0.83733  1721.90   \n2   0.17803 -0.006980    907.270  0.272140  0.45948   0.17327  2298.00   \n3   0.15236  0.007259    780.100  0.025179  0.51947   7.49140   112.51   \n4   0.11623  0.502900   -109.150  0.297910  0.34490  -0.40932  2538.90   \n\n          f8            f9        f10  ...     f110    f111     f112  \\\nid                                     ...                             \n0   168900.0  3.992400e+14     86.489  ... -12.2280  1.7482  1.90960   \n1   119810.0  3.874100e+15   9953.600  ... -56.7580  4.1684  0.34808   \n2   360650.0  1.224500e+13  15827.000  ...  -5.7688  1.2042  0.26290   \n3   259490.0  7.781400e+13    -36.837  ... -34.8580  2.0694  0.79631   \n4    65332.0  1.907200e+15    144.120  ... -13.6410  1.5298  1.14640   \n\n        f113      f114    f115          f116    f117     f118  claim  \nid                                                                    \n0   -7.11570   4378.80  1.2096  8.613400e+14   140.1  1.01770      1  \n1    4.14200    913.23  1.2464  7.575100e+15  1861.0  0.28359      0  \n2    8.13120  45119.00  1.1764  3.218100e+14  3838.2  0.40690      1  \n3  -16.33600   4952.40  1.1784  4.533000e+12  4889.1  0.51486      1  \n4   -0.43124   3856.50  1.4830 -8.991300e+12 -9999.0  0.23049      1  \n\n[5 rows x 118 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>f10</th>\n      <th>...</th>\n      <th>f110</th>\n      <th>f111</th>\n      <th>f112</th>\n      <th>f113</th>\n      <th>f114</th>\n      <th>f115</th>\n      <th>f116</th>\n      <th>f117</th>\n      <th>f118</th>\n      <th>claim</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.10859</td>\n      <td>0.004314</td>\n      <td>-37.566</td>\n      <td>0.017364</td>\n      <td>0.28915</td>\n      <td>-10.25100</td>\n      <td>135.12</td>\n      <td>168900.0</td>\n      <td>3.992400e+14</td>\n      <td>86.489</td>\n      <td>...</td>\n      <td>-12.2280</td>\n      <td>1.7482</td>\n      <td>1.90960</td>\n      <td>-7.11570</td>\n      <td>4378.80</td>\n      <td>1.2096</td>\n      <td>8.613400e+14</td>\n      <td>140.1</td>\n      <td>1.01770</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.10090</td>\n      <td>0.299610</td>\n      <td>11822.000</td>\n      <td>0.276500</td>\n      <td>0.45970</td>\n      <td>-0.83733</td>\n      <td>1721.90</td>\n      <td>119810.0</td>\n      <td>3.874100e+15</td>\n      <td>9953.600</td>\n      <td>...</td>\n      <td>-56.7580</td>\n      <td>4.1684</td>\n      <td>0.34808</td>\n      <td>4.14200</td>\n      <td>913.23</td>\n      <td>1.2464</td>\n      <td>7.575100e+15</td>\n      <td>1861.0</td>\n      <td>0.28359</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.17803</td>\n      <td>-0.006980</td>\n      <td>907.270</td>\n      <td>0.272140</td>\n      <td>0.45948</td>\n      <td>0.17327</td>\n      <td>2298.00</td>\n      <td>360650.0</td>\n      <td>1.224500e+13</td>\n      <td>15827.000</td>\n      <td>...</td>\n      <td>-5.7688</td>\n      <td>1.2042</td>\n      <td>0.26290</td>\n      <td>8.13120</td>\n      <td>45119.00</td>\n      <td>1.1764</td>\n      <td>3.218100e+14</td>\n      <td>3838.2</td>\n      <td>0.40690</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.15236</td>\n      <td>0.007259</td>\n      <td>780.100</td>\n      <td>0.025179</td>\n      <td>0.51947</td>\n      <td>7.49140</td>\n      <td>112.51</td>\n      <td>259490.0</td>\n      <td>7.781400e+13</td>\n      <td>-36.837</td>\n      <td>...</td>\n      <td>-34.8580</td>\n      <td>2.0694</td>\n      <td>0.79631</td>\n      <td>-16.33600</td>\n      <td>4952.40</td>\n      <td>1.1784</td>\n      <td>4.533000e+12</td>\n      <td>4889.1</td>\n      <td>0.51486</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.11623</td>\n      <td>0.502900</td>\n      <td>-109.150</td>\n      <td>0.297910</td>\n      <td>0.34490</td>\n      <td>-0.40932</td>\n      <td>2538.90</td>\n      <td>65332.0</td>\n      <td>1.907200e+15</td>\n      <td>144.120</td>\n      <td>...</td>\n      <td>-13.6410</td>\n      <td>1.5298</td>\n      <td>1.14640</td>\n      <td>-0.43124</td>\n      <td>3856.50</td>\n      <td>1.4830</td>\n      <td>-8.991300e+12</td>\n      <td>-9999.0</td>\n      <td>0.23049</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 118 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id=\"section-predict\"></a>\n## 4.Predict the missing values","metadata":{}},{"cell_type":"markdown","source":"In this method, we will predict/guess the value of a missing/null feature with the help of other non-null features. It is a more effective method when compared to other handling methods. This method may result in better accuracy. But, there is a problem with finding the correct predicting model. We need to try all the algorithms and pick the one with better accuracy. This is a good idea to try different algorithms instead of hoping for one.\n\n### Two ways:\n- 1. Model-based strategy \n- 2. Progressive model-based strategy\n\n#### Model-based strategy:\nLet fix a feature as a target feature train_y (having null values). Split this feature as having a null value set and having a non-null value set. The rows in the non-null value set makes the training data and the rows in the null values set makes the testing data. After this split, the data as train_x and train_y, train the model with this. Using this model we can find the value of the missing value in the target feature. see the example to get a clear understanding.\n#### Progressive model-based strategy:\nSame as above, but after we find the missing values in a given feature, we consider the feature as a predictor for predicting the missing values of the next feature.\n\nModels:\n1. linear regression.\n2. KNN\n3. logistric regression.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-model-based\"></a>\n### Model-based Imputation strategy","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values = np.nan, strategy = \"constant\", fill_value = -9999)\n\n# need some feature without null value , so creating one dataset randomly\ndef generate_data_predict_section(data, imputer):\n    data = data.iloc[11000:12000,30:45]\n    # impute some feature with constant to make dataset i want.\n    data.iloc[:,0:14] = pd.DataFrame(imputer.fit_transform(data.iloc[:,0:14]))\n    return data\ndata_predict_section  = generate_data_predict_section(data_,imputer)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:13:13.771185Z","iopub.execute_input":"2021-09-08T18:13:13.771505Z","iopub.status.idle":"2021-09-08T18:13:13.784386Z","shell.execute_reply.started":"2021-09-08T18:13:13.771474Z","shell.execute_reply":"2021-09-08T18:13:13.782797Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data_predict_section.isnull().sum()\n# this is the data i am going to use, 14  non-null features -- 1 null features.","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:13:15.230594Z","iopub.execute_input":"2021-09-08T18:13:15.230956Z","iopub.status.idle":"2021-09-08T18:13:15.240505Z","shell.execute_reply.started":"2021-09-08T18:13:15.230927Z","shell.execute_reply":"2021-09-08T18:13:15.239690Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"f31     0\nf32     0\nf33     0\nf34     0\nf35     0\nf36     0\nf37     0\nf38     0\nf39     0\nf40     0\nf41     0\nf42     0\nf43     0\nf44     0\nf45    18\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# method for Model-based strategy using Linear regression. i am taking only one null feature here. you can do the same for all other null features.\ndef model_based_imputation(data):\n    # find the null features.\n    feature_with_null = data.columns[data.isnull().any()].tolist()\n    # find the non-null feature.\n    feature_without_null = [col for col in list(data.columns) if col not in feature_with_null]\n    \n    # split train ans test data fro each null col and make prediction.\n    for col in feature_with_null:\n        test = data[data[col].isnull()]\n        train = data.dropna()\n        \n        # split as train_x and train_y \n        train_x, train_y = train[feature_without_null], train[feature_with_null]\n        test_x = test[feature_without_null]\n        \n        # train the model.\n        linear = LinearRegression().fit(train_x, train_y)\n        #predict the results\n        pred_y = linear.predict(test_x)\n        \n        # set back to dataset.\n        data.loc[test_x.index,feature_with_null] = pred_y\n        \n            \n    return data\n\ndata_after = model_based_imputation(data_predict_section)\ndata_after.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:13:16.249670Z","iopub.execute_input":"2021-09-08T18:13:16.250036Z","iopub.status.idle":"2021-09-08T18:13:16.514996Z","shell.execute_reply.started":"2021-09-08T18:13:16.250005Z","shell.execute_reply":"2021-09-08T18:13:16.513763Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"f31    0\nf32    0\nf33    0\nf34    0\nf35    0\nf36    0\nf37    0\nf38    0\nf39    0\nf40    0\nf41    0\nf42    0\nf43    0\nf44    0\nf45    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id = \"section-progressive-model-based\"></a>\n### Progressive model-based imputation strategy","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values = np.nan, strategy = \"constant\", fill_value = -9999)\n\n# need some feature without null value , so creating one dataset randomly\ndef generate_data_predict_section(data, imputer):\n    data = data.iloc[11000:12000,30:45]\n    # impute some feature with constant to make dataset i want.\n    data.iloc[:,0:10] = pd.DataFrame(imputer.fit_transform(data.iloc[:,0:10]))\n    return data\ndata_predict_section  = generate_data_predict_section(data_,imputer)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:13:18.439356Z","iopub.execute_input":"2021-09-08T18:13:18.439647Z","iopub.status.idle":"2021-09-08T18:13:18.452779Z","shell.execute_reply.started":"2021-09-08T18:13:18.439624Z","shell.execute_reply":"2021-09-08T18:13:18.450898Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data_predict_section.isnull().sum()\n# this is the data i am going to use, 10  non-null features -- 5 null features.","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:13:19.680789Z","iopub.execute_input":"2021-09-08T18:13:19.681353Z","iopub.status.idle":"2021-09-08T18:13:19.688117Z","shell.execute_reply.started":"2021-09-08T18:13:19.681325Z","shell.execute_reply":"2021-09-08T18:13:19.686717Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"f31     0\nf32     0\nf33     0\nf34     0\nf35     0\nf36     0\nf37     0\nf38     0\nf39     0\nf40     0\nf41    15\nf42    21\nf43    13\nf44    15\nf45    18\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# method for Model-based strategy using Linear regression. i am taking only one null feature here. you can do the same for all other null features.\ndef progressive_model_based_imputation(data):\n    # find the null features.\n    feature_with_null = data.columns[data.isnull().any()].tolist()\n    # find the non-null feature.\n    feature_without_null = [col for col in list(data.columns) if col not in feature_with_null]\n    #generate data without any null values\n    data_non_null = data[feature_without_null].copy()\n    data_null = data[feature_with_null].copy()\n    # split train ans test data fro each null col and make prediction.\n    for col in feature_with_null:\n        print(col)\n        # add the first null feature to data for predcition\n        data_non_null[col] = data_null[col]\n        test = data_non_null[data_non_null[col].isnull()]\n        train = data_non_null.dropna()\n        \n        # split as train_x and train_y \n        train_x, train_y = train[feature_without_null], train[col]\n        test_x = test[feature_without_null]\n        # train the model.\n        linear = LinearRegression().fit(train_x, train_y)\n        #predict the results\n        pred_y = linear.predict(test_x)\n        \n        # set back to dataset.\n        data_non_null.loc[test_x.index,col] = pred_y\n        #print(data_non_null.isnull().sum())\n        # add the col to feature_without_null list ---> because it has no null value now and it can be used for prediction as a training data.\n        feature_without_null.append(col)\n        #print(feature_without_null)\n\n    return data_non_null\n\ndata_after = progressive_model_based_imputation(data_predict_section)\nprint(\"The dataset after the progressive based imputation\")\ndata_after.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T18:13:20.812597Z","iopub.execute_input":"2021-09-08T18:13:20.812922Z","iopub.status.idle":"2021-09-08T18:13:20.880485Z","shell.execute_reply.started":"2021-09-08T18:13:20.812898Z","shell.execute_reply":"2021-09-08T18:13:20.879922Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"f41\nf42\nf43\nf44\nf45\nThe dataset after the progressive based imputation\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"f31    0\nf32    0\nf33    0\nf34    0\nf35    0\nf36    0\nf37    0\nf38    0\nf39    0\nf40    0\nf41    0\nf42    0\nf43    0\nf44    0\nf45    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id=\"section-model\"></a>\n## 5. Use models which support missing values","metadata":{}},{"cell_type":"markdown","source":"We can use some models which can supports null values. \nExample - KNN","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Completing your reading... ","metadata":{}},{"cell_type":"markdown","source":"### hope this will help some people like me :) please upvote if you find this helpful.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}